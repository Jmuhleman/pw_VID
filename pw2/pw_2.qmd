---
title: "Travail pratique VID"
subtitle: "TP #2" 
author:
  - name: Julien Muhlemann
    email: julien.muhlemann@heig-vd.ch
    affiliations:
      - name: Département TIC
highlight-style: github
format:
  html:
    theme: cosmo
    monobackgroundcolor: rgb(255,230,240)
    backgroundcolor: rgb(250,250,250)
    toc: true
    toc-location: left
    #reference-location: margin
    reference-location: document
    code-line-numbers: true
  pdf:
    pdf-engine: pdflatex
    toc: true
    number-sections: false
    colorlinks: true
    
date: 'last-modified'
date-format: 'D MMMM, YYYY'
number-sections: false
editor: 
  visual
---






description des données, 
calligraphie latek
précisions dans explications
PAS d'ENGLISH
PLUS de commentaires



# Introduction

blablabla




# Exercice 1

## a  Calculer le coefficient de corrélation entre la taille et l'age.

```{r, warning=FALSE, results=TRUE, fig.height=10, fig.width=10}
kalama <- read.table("kalama.txt", header = TRUE, sep = "", dec = ".")

print(kalama)
```

```{r, warning=FALSE, results=TRUE, fig.height=10, fig.width=10}


print(coeff_correlation <- cor(kalama$taille, kalama$age, method = "pearson"))

```

## b  Tracer le nuage de points taille versus age.

```{r, warning=FALSE, results=TRUE, fig.height=10, fig.width=10}

plot(kalama)

```

## c Estimer les coefficients $\beta_{0}$ et $\beta_{1}$ par la méthode des moindres carrés. Enregistrer le résultat de l'ajustement du modèle dans l'objet kalama.lm de R.

```{r, warning=FALSE, results=TRUE, fig.height=10, fig.width=10}


kalama.lm <- lm(age ~ taille, data=kalama)
#par(mfrow=c(2,2), pty="s")
#plot(kalama.lm, which=1:4)
```

```{r, warning=FALSE, results=TRUE, fig.height=10, fig.width=10}
kalama.lm
```

Donc nous avons:

$\beta_{0}$ = -100.842 et $\beta_{1}$ = 1.557

## d Ajuster sur le graphique la droite des moindres carrés en utilisant la fonction abline().

```{r, warning=FALSE, results=TRUE, fig.height=10, fig.width=10}


coeff <- coefficients(kalama.lm)

eq <- paste0("y = ", round(coeff[2], 1), " * x ", 
            ifelse(coeff[1] >= 0, " + ", " - "), 
            abs(round(coeff[1], 1)))

plot(kalama$taille, kalama$age, main = eq, 
     xlab = "Taille", ylab = "age") 

abline(kalama.lm, col = "blue", lwd = 2)
```

## e Estimer la variance des erreurs $\sigma^2$.

```{r, warning=FALSE, results=TRUE, fig.height=10, fig.width=10}

kalama.lm <- lm(age ~ taille, data=kalama)
par(mfrow=c(1,1), pty="s")
plot(kalama.lm, which=1)

```

```{r, warning=FALSE, results=TRUE, fig.height=10, fig.width=10}

model_summary <- summary(kalama.lm)
mse <- model_summary$sigma^2
#model_summary
print(mse)
```

## f Déterminer la valeur du coefficient de détermination R2 de la régression.

```{r, warning=FALSE, results=TRUE, fig.height=10, fig.width=10}

model_summary <- summary(kalama.lm)
r_squared <- model_summary$r.squared
print(r_squared)

```
## g Effectuer un diagnostic du modèle ajusté à l’aide des graphiques appropriés.

```{r, warning=FALSE, results=TRUE, fig.height=10, fig.width=10}
library(ggResidpanel)
resid_panel(kalama.lm, plots="all")
resid_interact(kalama.lm)

```

## h En combinant les résultats obtenus en d), f) et g), qualifier l’ajustement du modèle aux
données observées.

Nous pouvons dire que le modèle est bien ajusté aux données observées. En effet, le coefficient de détermination est de 0.988, ce qui signifie que 98% de la variance de la variable dépendante est expliquée par la variable indépendante. De plus, les graphiques de diagnostic montrent que les résidus sont relativement bien distribués autour de 0 et ne présentent pas de structure particulière.

Nous pouvons observer dans le Quantile-Quantile plot (QQ plot) que les résidus suivent une distribution proche de la loi normale. Ce que confirme l'histogramme des résidus et la boite à moustache sur la même ligne.

La distance de Cook nous permet de visualiser les observations qui ont le plus d'impact sur le modèle. Nous pouvons voir que les observations 1 et 2 ont un impact plus important que les autres observations. Ceci est certainement dû au fait que ces deux observations sont des valeurs aberrantes.

Nous pouvons observer le graphique de la régression en haut à droite. Celui-ci nous indique que le modèle de manière générale est bien adapté à faire des prédictions sur de nouvelles données. Cependant, il est important de noter que nous disposons de relativement peu d'observations donc il est possible que le modèle ne soit pas aussi robuste avec de nouvelles données.





## i Tester à un niveau de signification de 5% si la pente de la droite de régression est significativement
différente de zéro.


```{r, warning=FALSE, results=TRUE, fig.height=10, fig.width=10}

summary_result <- summary(kalama.lm)
attributes(summary_result$coefficients)

p_value <- summary_result$coefficients[2, 4] 
print(p_value)

```


Nous avons une p-valeur de ~4.43E-11 qui est inférieure à 0.05. Nous pouvons donc rejeter l'hypothèse nulle selon laquelle la pente de la droite de régression est égale à zéro. Nous pouvons donc conclure que la pente de la droite de régression est significativement différente de zéro.















